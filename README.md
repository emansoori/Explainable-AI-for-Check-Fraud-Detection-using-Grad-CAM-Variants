# Explainable AI for Check Fraud Detection using Grad-CAM Variants
 This project explores the robustness of different Grad-CAM variants (Grad-CAM, Grad-CAM++, XGrad-CAM, and Score-CAM) for explainable deep learning in check fraud detection. Using a CNN model based on VGG16, we classify real vs. fraudulent checks and generate class activation heatmaps to interpret model decisions. The robustness of the visual explanations is evaluated under five levels of Gaussian noise using structural similarity (SSIM), intersection-over-union (IoU), and confidence drop metrics. Results demonstrate that Score-CAM outperforms other methods in maintaining interpretability under noisy conditions
